"""
This script downloads a list of GeoJSON file entries from NOAA SWPC's experimental product.
It has four modes:
  1. Normal CZML mode:
     - Using a user-specified target date/time, it finds the file whose time_tag is closest to that target,
       then processes that file and up to NUM_URLS files sequentially.
     - For each file, it downloads the GeoJSON, extracts each feature’s coordinates and hmF2 value to create a vertical polyline,
       and adds a "show" block so that the polyline is visible from the file’s time_tag until 10 minutes later.
     - A document CZML packet is created with a clock entry covering the overall interval.
  2. -alldb mode:
     - All files in the listing are processed.
     - For each file and for each feature in that file, a CSV row is written (and the same data inserted into a SQLite database)
       containing the following columns:
         timestamp, longitude, latitude, hmF2, NmF2, quality_flag
       where uid is automatically generated by SQLite.
  3. -updatedb <timestamp> mode:
     - The script accepts a timestamp (formatted as 2025-02-03T00:45:00Z) and processes all geojson entries
       whose time_tag is later than the provided timestamp, appending new records into glotec.db.
  4. -appnd_ncd <netcdf_link> mode:
     - The script downloads a netCDF4 file from the provided URL. The file is expected to have variables:
         time (shape: 144),
         latitude (shape: 72),
         longitude (shape: 72),
         hmF2 (shape: (144, 72, 72)), and
         quality_flag (shape: (144, 72, 72)).
       It loops through the time dimension and for each grid cell creates a record with columns:
         timestamp, longitude, latitude, hmF2, NmF2, quality_flag,
       and appends each record to the existing glotec.db database.
  5. -nmpatch <start_timestamp> <end_timestamp> mode:
     - The script accepts two timestamps (formatted as 2025-02-03T00:45:00Z) defining a date range.
       It deletes all rows in the glotec table whose timestamp is within that range, and then replaces them
       by processing the corresponding files from the NOAA listing (including the related NmF2 values).
  6. -laglotec mode:
     - The script retrieves only the most recent glotec entry from the listing and creates a new glotec database
       containing only the records from that most recent GeoJSON file.
"""

import sys
import json
import requests
import csv
import sqlite3
import math
import tempfile
import numpy as np
from datetime import datetime, timedelta
from netCDF4 import Dataset, num2date

# -------------------------
# User-adjustable parameters:
NUM_URLS = 30  # maximum number of URLs to process (for CZML mode)

# Specify the target date/time (in UTC) to start with (for CZML mode).
target_year = 2025
target_month = 1
target_day = 2
target_hour = 4
target_minute = 0
target_dt = datetime(target_year, target_month, target_day, target_hour, target_minute)

# Base URL to prepend to relative URLs from the file list
BASE_URL = "https://services.swpc.noaa.gov/"

# URL to the JSON file that lists the GeoJSON file entries
LISTING_URL = "https://services.swpc.noaa.gov/products/glotec/geojson_2d_urt.json"


# -------------------------
# Function for "all database dump" mode.
def dump_all_data():
    print("Running in -alldb mode: dumping all data into CSV and SQLite database.")
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    if not full_listing:
        print("No entries found in the listing.")
        sys.exit(1)

    csv_filename = "glotec.csv"
    db_filename = "glotec.db"
    csv_fieldnames = ["timestamp", "tec", "anomaly", "longitude", "latitude", "hmF2", "NmF2", "quality_flag", "uid"]

    try:
        csv_file = open(csv_filename, "w", newline='')
        csv_writer = csv.DictWriter(csv_file, fieldnames=csv_fieldnames)
        # Uncomment the next line if you want to write a header:
        # csv_writer.writeheader()
    except Exception as e:
        print(f"Error opening CSV file {csv_filename}: {e}")
        sys.exit(1)

    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS glotec (
                uid INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                longitude REAL,
                latitude REAL,
                hmF2 INTEGER,
                NmF2 REAL,
                quality_flag TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error setting up SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_rows = 0

    for entry in full_listing:
        try:
            file_time = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue
        full_url = BASE_URL + rel_url
        print("Processing file: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                uid = cur.lastrowid
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

            total_rows += 1

    conn.commit()
    conn.close()
    csv_file.close()

    print(f"Dumped data for {total_rows} features into {csv_filename} and {db_filename}.")


# -------------------------
# Function for updating the database from geojson entries after a given timestamp.
def update_db(timestamp_input):
    """
    Accepts a timestamp string in ISO 8601 format (e.g., 2025-02-03T00:45:00Z). It fetches all
    entries from the listing whose time_tag is later than the provided timestamp and appends
    their data to the existing glotec.db database.
    """
    try:
        update_dt = datetime.strptime(timestamp_input, "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing provided timestamp {timestamp_input}: {e}")
        sys.exit(1)

    print(f"Updating database with entries later than {timestamp_input}.")

    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    filtered_entries = []
    for entry in full_listing:
        try:
            file_dt = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        if file_dt > update_dt:
            filtered_entries.append(entry)

    if not filtered_entries:
        print("No new entries found after the provided timestamp.")
        sys.exit(0)

    print(f"Found {len(filtered_entries)} new entries to process.")

    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
    except Exception as e:
        print(f"Error opening SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_new_rows = 0

    for entry in filtered_entries:
        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue
        full_url = BASE_URL + rel_url
        print("Processing file: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                uid = cur.lastrowid
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

            total_new_rows += 1

    conn.commit()
    conn.close()

    print(f"Updated database with {total_new_rows} new rows.")


# -------------------------
# Function for updating the database from a netCDF4 file.
def update_db_from_ncd(netcdf_link):
    """
    Accepts a URL to a netCDF4 file. The netCDF file is expected to contain variables:
      - time (shape: 144)
      - latitude (shape: 72)
      - longitude (shape: 72)
      - hmF2 (shape: (144, 72, 72))
      - quality_flag (shape: (144, 72, 72))
    where the hmF2 and quality_flag variables have dimensions ('time', 'latitude', 'longitude').
    The function downloads the netCDF file, loops through each time index and each grid cell,
    creates a record with columns:
       timestamp, longitude, latitude, hmF2, NmF2, quality_flag,
    and appends each record to the existing glotec.db database.
    For netCDF files (which do not contain a NmF2 variable), NmF2 is stored as NULL.
    """
    print(f"Updating database from netCDF file: {netcdf_link}")
    try:
        r = requests.get(netcdf_link)
        r.raise_for_status()
    except Exception as e:
        print(f"Error downloading netCDF file from {netcdf_link}: {e}")
        sys.exit(1)

    # Write the downloaded netCDF content to a temporary file.
    with tempfile.NamedTemporaryFile(delete=False) as tmp:
        tmp.write(r.content)
        tmp.flush()
        tmp_filename = tmp.name

    try:
        ds = Dataset(tmp_filename, 'r')
    except Exception as e:
        print(f"Error opening netCDF file: {e}")
        sys.exit(1)

    try:
        time_var = ds.variables["time"][:]      # shape (144,)
        lat_var = ds.variables["latitude"][:]     # shape (72,)
        lon_var = ds.variables["longitude"][:]      # shape (72,)
        hmF2_var = ds.variables["hmF2"][:]          # shape (144, 72, 72)
        qflag_var = ds.variables["quality_flag"][:]  # shape (144, 72, 72)
        # Convert the time variable to datetime objects using its units.
        time_units = ds.variables["time"].units
        time_calendar = ds.variables["time"].calendar if "calendar" in ds.variables["time"].ncattrs() else "standard"
        times = num2date(time_var, units=time_units, calendar=time_calendar)
    except Exception as e:
        print(f"Error retrieving variables from netCDF: {e}")
        ds.close()
        sys.exit(1)

    ds.close()

    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
    except Exception as e:
        print(f"Error opening SQLite database {db_filename}: {e}")
        sys.exit(1)

    total_new_rows = 0
    n_time = len(times)
    n_lat = len(lat_var)
    n_lon = len(lon_var)

    print(f"Processing netCDF data with {n_time} time steps, {n_lat} latitudes, {n_lon} longitudes.")
    for t in range(n_time):
        timestamp_str = times[t].strftime("%Y-%m-%dT%H:%M:%SZ")
        for i in range(n_lat):
            for j in range(n_lon):
                lat_val = float(lat_var[i])
                lon_val = float(lon_var[j])
                hmF2_val = hmF2_var[t, i, j]
                # Replace any masked element with nan.
                hmF2_val_filled = np.ma.filled(hmF2_val, np.nan)
                if np.isnan(hmF2_val_filled):
                    hmF2_int = None
                else:
                    hmF2_int = math.trunc(float(hmF2_val_filled))
                quality_flag = str(qflag_var[t, i, j])
                # For netCDF updates, there is no NmF2 variable; store as NULL (None).
                NmF2_val = None
                try:
                    cur.execute("""
                        INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (timestamp_str, lon_val, lat_val, hmF2_int, NmF2_val, quality_flag))
                    total_new_rows += 1
                except Exception as e:
                    print(f"Error inserting record for time {timestamp_str}, lat {lat_val}, lon {lon_val}: {e}")
                    continue

    conn.commit()
    conn.close()

    print(f"Updated database from netCDF file: {total_new_rows} new rows inserted.")


# -------------------------
# New function for NM patch.
def nm_patch(start_timestamp, end_timestamp):
    """
    Accepts two timestamp strings in ISO 8601 format (e.g., 2025-02-03T00:45:00Z and 2025-02-03T01:00:00Z).
    This function will:
      1. Delete all rows in the glotec table whose timestamp is between start_timestamp and end_timestamp (inclusive).
      2. Fetch the NOAA listing from LISTING_URL, filter for entries whose time_tag falls within that range,
         and process each such file (including retrieving NmF2 values) to reinsert the corresponding data.
    """
    try:
        start_dt = datetime.strptime(start_timestamp, "%Y-%m-%dT%H:%M:%SZ")
        end_dt = datetime.strptime(end_timestamp, "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing provided timestamps: {e}")
        sys.exit(1)

    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
    except Exception as e:
        print(f"Error opening SQLite database {db_filename}: {e}")
        sys.exit(1)

    # Delete rows with timestamp between start_timestamp and end_timestamp.
    try:
        cur.execute("DELETE FROM glotec WHERE timestamp >= ? AND timestamp <= ?;", (start_timestamp, end_timestamp))
        deleted = cur.rowcount
        conn.commit()
        print(f"Deleted {deleted} rows from glotec between {start_timestamp} and {end_timestamp}.")
    except Exception as e:
        print(f"Error deleting rows: {e}")
        conn.close()
        sys.exit(1)

    # Now fetch the listing and filter for entries within the date range.
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        full_listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        conn.close()
        sys.exit(1)

    filtered_entries = []
    for entry in full_listing:
        try:
            file_dt = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
        except Exception as e:
            print(f"Error parsing time_tag {entry.get('time_tag')}: {e}")
            continue

        if start_dt <= file_dt <= end_dt:
            filtered_entries.append(entry)

    if not filtered_entries:
        print("No entries found in the listing within the provided date range.")
        conn.close()
        sys.exit(0)

    print(f"Found {len(filtered_entries)} entries to patch.")

    total_new_rows = 0

    for entry in filtered_entries:
        timestamp_str = entry["time_tag"]

        rel_url = entry.get("url")
        if not rel_url:
            print("Skipping an entry because it has no 'url' field.")
            continue

        full_url = BASE_URL + rel_url
        print("Processing file for NM patch: " + full_url)

        try:
            r = requests.get(full_url)
            r.raise_for_status()
            geojson_data = r.json()
        except Exception as e:
            print(f"Error downloading GeoJSON from {full_url}: {e}")
            continue

        features = geojson_data.get("features", [])
        for i, feature in enumerate(features):
            properties = feature.get("properties", {})
            geometry = feature.get("geometry", {})

            if geometry.get("type") != "Point":
                print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
                continue

            coords = geometry.get("coordinates", [])
            if len(coords) < 2:
                print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
                continue

            longitude, latitude = coords[0], coords[1]

            hmF2 = properties.get("hmF2")
            NmF2 = properties.get("NmF2")
            quality_flag = properties.get("quality_flag")

            try:
                hmF2 = math.trunc(float(hmF2))
            except Exception:
                print("hmF2 failed " + str(hmF2))
                hmF2 = None

            try:
                NmF2 = float(NmF2) if NmF2 is not None else None
            except Exception:
                print("NmF2 conversion failed " + str(NmF2))
                NmF2 = None

            try:
                cur.execute("""
                    INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
                total_new_rows += 1
            except Exception as e:
                print(f"Error inserting row into database: {e}")
                continue

    conn.commit()
    conn.close()

    print(f"NM patch complete: {total_new_rows} new rows inserted for entries between {start_timestamp} and {end_timestamp}.")


# -------------------------
# New function for -laglotec mode.
def latest_glotec():
    """
    Retrieves only the most recent glotec entry from the listing and creates a new glotec database
    containing only the records from that single entry.
    """
    print("Running in -laglotec mode: creating new glotec database from the most recent entry.")
    try:
        r = requests.get(LISTING_URL)
        r.raise_for_status()
        listing = r.json()
    except Exception as e:
        print(f"Error downloading listing JSON: {e}")
        sys.exit(1)

    if not listing:
        print("No entries found in the listing.")
        sys.exit(1)

    # Find the entry with the maximum (most recent) time_tag.
    try:
        for entry in listing:
            entry["time_dt"] = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing time_tag in listing: {e}")
        sys.exit(1)

    latest_entry = max(listing, key=lambda e: e["time_dt"])
    print(f"Most recent entry found: {latest_entry['time_tag']}")

    # Create (or overwrite) the glotec database.
    db_filename = "glotec.db"
    try:
        conn = sqlite3.connect(db_filename)
        cur = conn.cursor()
        cur.execute("DROP TABLE IF EXISTS glotec;")
        cur.execute("""
            CREATE TABLE glotec (
                uid INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT,
                longitude REAL,
                latitude REAL,
                hmF2 INTEGER,
                NmF2 REAL,
                quality_flag TEXT
            )
        """)
        conn.commit()
    except Exception as e:
        print(f"Error setting up new SQLite database {db_filename}: {e}")
        sys.exit(1)

    # Process only the latest entry.
    timestamp_str = latest_entry["time_tag"]
    rel_url = latest_entry.get("url")
    if not rel_url:
        print("The most recent entry has no 'url' field.")
        conn.close()
        sys.exit(1)
    full_url = BASE_URL + rel_url
    print("Processing most recent file: " + full_url)

    try:
        r = requests.get(full_url)
        r.raise_for_status()
        geojson_data = r.json()
    except Exception as e:
        print(f"Error downloading GeoJSON from {full_url}: {e}")
        conn.close()
        sys.exit(1)

    total_rows = 0
    features = geojson_data.get("features", [])
    for i, feature in enumerate(features):
        properties = feature.get("properties", {})
        geometry = feature.get("geometry", {})

        if geometry.get("type") != "Point":
            print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
            continue

        coords = geometry.get("coordinates", [])
        if len(coords) < 2:
            print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
            continue

        longitude, latitude = coords[0], coords[1]

        hmF2 = properties.get("hmF2")
        NmF2 = properties.get("NmF2")
        quality_flag = properties.get("quality_flag")

        try:
            hmF2 = math.trunc(float(hmF2))
        except Exception:
            print("hmF2 failed " + str(hmF2))
            hmF2 = None

        try:
            NmF2 = float(NmF2) if NmF2 is not None else None
        except Exception:
            print("NmF2 conversion failed " + str(NmF2))
            NmF2 = None

        try:
            cur.execute("""
                INSERT INTO glotec (timestamp, longitude, latitude, hmF2, NmF2, quality_flag)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (timestamp_str, longitude, latitude, hmF2, NmF2, quality_flag))
            total_rows += 1
        except Exception as e:
            print(f"Error inserting row into database: {e}")
            continue

    conn.commit()
    conn.close()
    print(f"Created new glotec database with {total_rows} rows from the most recent entry.")


# -------------------------
# Main script execution.
if "-alldb" in sys.argv:
    dump_all_data()
    sys.exit(0)

if "-updatedb" in sys.argv:
    try:
        idx = sys.argv.index("-updatedb")
        timestamp_arg = sys.argv[idx + 1]
    except IndexError:
        print("Error: -updatedb option requires a timestamp argument formatted as 2025-02-03T00:45:00Z")
        sys.exit(1)
    update_db(timestamp_arg)
    sys.exit(0)

if "-appnd_ncd" in sys.argv:
    try:
        idx = sys.argv.index("-appnd_ncd")
        netcdf_link = sys.argv[idx + 1]
    except IndexError:
        print("Error: -appnd_ncd option requires a netCDF4 file URL as an argument.")
        sys.exit(1)
    update_db_from_ncd(netcdf_link)
    sys.exit(0)

if "-nmpatch" in sys.argv:
    try:
        idx = sys.argv.index("-nmpatch")
        start_patch = sys.argv[idx + 1]
        end_patch = sys.argv[idx + 2]
    except IndexError:
        print("Error: -nmpatch option requires two timestamp arguments formatted as 2025-02-03T00:45:00Z")
        sys.exit(1)
    nm_patch(start_patch, end_patch)
    sys.exit(0)

if "-laglotec" in sys.argv:
    latest_glotec()
    sys.exit(0)

# --- Normal CZML mode below ---
try:
    r = requests.get(LISTING_URL)
    r.raise_for_status()
    listing = r.json()
except Exception as e:
    print(f"Error downloading listing JSON: {e}")
    sys.exit(1)

if not listing:
    print("No entries found in the listing.")
    sys.exit(1)

for entry in listing:
    try:
        entry["time_dt"] = datetime.strptime(entry["time_tag"], "%Y-%m-%dT%H:%M:%SZ")
    except Exception as e:
        print(f"Error parsing time_tag {entry.get('time_tag')} : {e}")
        sys.exit(1)

sorted_listing = sorted(listing, key=lambda e: e["time_dt"])
closest_entry = min(sorted_listing, key=lambda e: abs(e["time_dt"] - target_dt))
closest_index = sorted_listing.index(closest_entry)

print(f"Target datetime: {target_dt}")
print(f"Closest file time_tag: {closest_entry['time_tag']}")

entries = sorted_listing[closest_index:closest_index + NUM_URLS]
if len(entries) == 0:
    print("No entries available after the target time.")
    sys.exit(1)

interval_delta = timedelta(minutes=10)
start_times = [entry["time_dt"] for entry in entries]
end_times = [entry["time_dt"] + interval_delta for entry in entries]

overall_start = min(start_times)
overall_end = max(end_times)

clock_interval = f"{overall_start.strftime('%Y-%m-%dT%H:%M:%SZ')}/{overall_end.strftime('%Y-%m-%dT%H:%M:%SZ')}"

color_scale = [
    {"rgba": [0, 0, 0, 255]},
    {"rgba": [165, 42, 42, 255]},
    {"rgba": [255, 0, 0, 255]},
    {"rgba": [255, 165, 0, 255]},
    {"rgba": [255, 255, 0, 255]},
    {"rgba": [0, 128, 0, 255]},
    {"rgba": [0, 0, 255, 255]},
    {"rgba": [238, 130, 238, 255]},
    {"rgba": [255, 255, 255, 255]}
]
n_colors = len(color_scale)

min_alt_km = 150.0
max_alt_km = 500.0
range_alt = max_alt_km - min_alt_km

def get_color_for_altitude(hmF2_km):
    if hmF2_km < min_alt_km:
        hmF2_km = min_alt_km
    elif hmF2_km > max_alt_km:
        hmF2_km = max_alt_km

    normalized = (hmF2_km - min_alt_km) / range_alt
    idx = int(normalized * (n_colors - 1))
    return color_scale[idx]

czml = []

document_packet = {
    "id": "document",
    "name": "Ionospheric hmF2 Paths Animated",
    "version": "1.0",
    "clock": {
        "interval": clock_interval,
        "currentTime": overall_start.strftime('%Y-%m-%dT%H:%M:%SZ')
    }
}
czml.append(document_packet)

for entry in entries:
    rel_url = entry.get("url")
    if not rel_url:
        print("Skipping an entry because it has no 'url' field.")
        continue
    full_url = BASE_URL + rel_url
    print("processing " + full_url)

    start_time = entry["time_dt"]
    end_time = start_time + interval_delta
    show_interval = f"{start_time.strftime('%Y-%m-%dT%H:%M:%SZ')}/{end_time.strftime('%Y-%m-%dT%H:%M:%SZ')}"

    try:
        r = requests.get(full_url)
        r.raise_for_status()
        geojson_data = r.json()
    except Exception as e:
        print(f"Error downloading GeoJSON from {full_url}: {e}")
        continue

    features = geojson_data.get("features", [])
    for i, feature in enumerate(features):
        properties = feature.get("properties", {})
        geometry = feature.get("geometry", {})

        if geometry.get("type") != "Point":
            print(f"Skipping feature {i} in file {rel_url}: geometry type is not 'Point'.")
            continue

        coords = geometry.get("coordinates", [])
        if len(coords) < 2:
            print(f"Skipping feature {i} in file {rel_url}: insufficient coordinate data.")
            continue

        lon, lat = coords[0], coords[1]
        hmF2 = properties.get("hmF2")
        if hmF2 is None:
            print(f"Skipping feature {i} in file {rel_url}: no hmF2 value found.")
            continue
        try:
            hmF2 = float(hmF2)
        except ValueError:
            print(f"Skipping feature {i} in file {rel_url}: hmF2 value is not numeric.")
            continue

        alt_m = hmF2 * 1000.0
        color = get_color_for_altitude(hmF2)
        positions = [lon, lat, 0, lon, lat, alt_m]
        entity_id = f"{entry['time_tag']}_feature_{i}"
        entity = {
            "id": entity_id,
            "name": f"hmF2: {hmF2} km, time: {entry['time_tag']}",
            "polyline": {
                "positions": {
                    "cartographicDegrees": positions
                },
                "material": {
                    "solidColor": {
                        "color": color
                    }
                },
                "width": 2,
                "show": [
                    {
                        "interval": show_interval,
                        "boolean": True
                    }
                ]
            }
        }
        czml.append(entity)

output_filename = "animated_output.czml"
try:
    with open(output_filename, "w") as f:
        json.dump(czml, f, indent=2)
    print(f"CZML file successfully written to '{output_filename}'.")
except Exception as e:
    print(f"Error writing CZML file: {e}")
